# Overview

The following is a first pass at a description of this project, what problem
we're solving, and how we're solving it. This document will almost certainly be
stale by the time anyone reads it, but is a good starting point.

# Problem Domain

The system we are building is an _analytical engine_; it is a system that
receives an input, generates an analysis of the input, then executes the
analysis. The fundamental engineering problem we are solving is to build a
an analytical engine that is _high quality_, _economic_, _observable_, and _iterable_.

The system is _high quality_ if the outputs from its analyses and executions are
correct and useful. General speaking, _quality_ is the metric that determines
end user satisfaction.

The system is _economic_ if it is able to perform its tasks by providing more
value than the cost of its inputs. _economy_ is the metric that determines
business viability and ultimate success.

The system is _observable_ if we can understand and report on its behavior and
state. Generally speaking, _observability_ is the metric that determines
administrator satisfaction. _Observability_ is also a very important
prerequisite for _iterability_.

The system is _iterable_ if we can change its behavior and state in a controlled
manner. Generally speaking, _iterability_ is the metric that determines
developer satisfaction.

# Engineering Challenges

An engine is bootstrapped with a suite of primitive operators (primitive for
short). A _primitive_ is a function with a language model binding. A _langauge
model binding_ is a construct that exposes an API to a language model. **The
first engineering challenge is to build a suite of useful primitives.**

The engine receives an input and then decides how to map that input onto its
suite of primitives. This mapping is called an analysis. An _analysis_ is a tree
structure where each node is a primitive. An analysis can be _executed_ on an
input. When we execute an analysis, we execute each primitive in the analysis in
order. The output of each primitive can be used as the input to the next one.
The result of executing an analysis is called a _trajectory_. A useful analysis
is an analysis that generates a useful trajectory. **The second engineering
problem is to generate useful analyses.**

The engine receives input, generates an analysis, and then executes the
analysis using its suite of primitives. Inputs are assumed to be natural
language user input. Analysis generation, execution, and primitives
are all assumed to be extremely unreliable and somewhat nondeterministic.
Analysis and execution are both recursive operations because primitives may
themselves be engines.The boundary between generation and execution is
somewhat fuzzy because the generation process may involve some execution of an
partial analysis and execution may involve generating ephemeral analyses.
Runtime annealing throws additional complexity into the mix.

So, what we have is a system that

- takes natural language as input,
- with primitive operations that are extremely expensive and unreliable,
- that is recursive in nature,
- whose inputs and outputs are interwoven,
- and whose behavior must be modified at runtime.

Said another way: we have an engineer's nightmare. To be successful we must
build tools that make the system easier to build, operate, and evaluate. **The
third engineering challenge is to bootstrap a development and operations
nvironment that accounts for the system's particular complexities.**

# Building Useful Primitives

TODO

# Generating Useful Analyses

Our solution draws heavily from the feild of reinforcement learning. In
particular, we will be implenting one or more variants of [Monte Carlo Tree
Search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search). We can think of
the persistent search tree as the engine's _memory_. The idea is that
the more we run the engine, the smarter and more efficient the engine becomes,
and the more useful the analyses it generates.

A familiarity with MCTS, its components, and its variants is a prerequisite for
understanding the rest of this section and, ultimately, for development. To
implement MCTS we will need to define its _environment_ with respect to our
problem domain and create implementations of its _selection_, _expansion_,
_simulation_, and _backpropagation_ components.

## Environment

In general, MCTS is a way to generate a winning strategy for a game. The set of
all possible states for a game is called the universe. Every state in the
universe has a value between 0 and 1. If the value is 0, the game is lost. If
the value is 1, the game is won. Every state in the universe has a set of
actions that can be taken from that state and each action yields a new state.

So the first step in applying MCTS to our problem domain is to define the game
we are playing. In particular:

- what is the universe?
- what are the actions that can be taken from each state?
- what is the value of each state?

The input domain we care about right now is _natural language questions_, i.e.
**the universe is the set of all natural language questions and all possible
analyses of those questions**. This set is in turn defined by the suite
of primitives that are available to the engine (because an analysis is just a
tree of primitives to apply to a question). **The engine's primitives therefore
correspond to the game's actions**. The value of each state is **a measure of its
corresponding analysis's ability to generate a useful trajectory**.

## Selection

The selection component of MCTS is responsible for selecting the next state to
explore. The critical aspect of selection is that it balances exploration and
exploitation. In other words, it balances the desire to explore known good
states and the desire to explore unknown states. In our case, the selection
component is responsible for selecting the next primitive to add to the current
analysis.

**Open Questions**

- How do we define the value of a state?
- Assuming we have defined the value of a state, what algorithm should we use to
  select the next state to explore?
- How do we deal with different questions? Do we have a different search tree
  for each question? Do we have a different search tree for each question type?
  Do we have a single search tree for all questions where one of the actions we
  can take somehow selects a question type?
- What is the simplest workable version of this component?

## Expansion

The expansion component of MCTS is responsible for generating the next state to
explore. In our case, the expansion component is responsible for generating the
next primitive to add to the current analysis.

**Open Questions**

- This seems like the most straightforward component to implement. Is there
  anything we need to consider here?
- What is the simplest workable version of this component?

## Simulation

The simulation component of MCTS is responsible for generating
information. It does this by exploring potential outcomes resulting from the
current state. In our case, the simulation component will likely be mostly
concerned with evaluating the current analysis.

**Open Questions**

- What exactly do we want to generate here? Do we want to randomly select
  actions to take beyond the current analysis? Do we want to just execute the
  current analysis? Do we want to execute the current analysis but with
  different prompts? Do we ant to execute the current analysis but with
  different configuration (e.g. temperature)?
- How do we evaluate a trajectory?
- What is the simplest workable version of this component?

## Backpropagation

The backpropagation component of MCTS is responsible for incorporating
simulation results back into the search tree.

**Open Questions**

- What is the algorithm for incorporating analysis results back into the search
  tree? Can we just apply the UCB1 formula?
- What is the simplest workable version of this component?

# Bootstrapping Development and Operations

First, it's important to consider the general architecture of the system and how
it impacts the tools we require. First, let's think about the engine and how
we'll need to work with it.

- Everything about the engine is unreliable
- Everything about the engine is qualitative
- The engine's outputs are dependent on prior outputs (i.e. it is extremely
  stateful)

So what we need is a very, very strong data model and a very, very tight feedback
loop. We basically need a REPL for the engine that retains all of its state and
version history.

Additionally, and maybe most importantly, is we will need to be able to _run
experiments_. This means we need to be able to execute runs, inspect runs,
record their outputs, evaluate their outputs, twiddle their inputs, and repeat.

**Open Questions**

- What are the specific tools we need to build?
- What are the most important tools to build first?
- What are the simplest workable versions of these tools?
- Some of these tools will be used by developers, some will be used by
  administrators, and some will be used by end users. How do we prioritize
  these tools?
- Some random words that correspond to tools in my head:
  - REPL
  - testing
  - evaluation
  - human in the loop
  - neuroplasticity
  - analytics
  - time travel
  - guardrails
  - performance

# Implementation Details

**High Level Workflow**

- write an analyzer function, export it from ./analyzers
- publish the analyzer to the database
- write an evaluator function, export it from ./evaluators
- publish the evaluator to the database
- write a reducer function, export it from ./reducers
- publish the reducer to the database
- write one or more synthesizer functions, export them from ./synthesizers
- publish the synthesizers to the database
- write an engine spec, export it from ./engines
- publish the engine spec to the database
- run a trajectory using the /generate endpoint
- stop a trajectory using the /kill endpoint

**Primitives Worklow**

_author / publish / read / compile / execute_

The analytical engine allows a language model to write programs. Just like in a
normal programming language, the program is a sequence of subroutine calls. We
use the term "primitive" to refer to a subroutine call. Language models can't
directly execute code, so we have implemented a system for exposing functions to
the language models. The system involves the following steps:

- author: write a function using TypeScript, commit it to the repository
- publish: write the functions metadata to the database
- read: when the language model requests a primitive, we read the primitive's
  metadata from the database
- compile: we use the program's metadata to route the request to the correct
  TypeScript function
- execute: we execute the function by calling it with the arguments provided by
  the language model

# Useful Resources

## Tree Search

- https://en.wikipedia.org/wiki/Monte_Carlo_tree_search
- https://en.wikipedia.org/wiki/Minimax
- https://en.wikipedia.org/wiki/Multi-armed_bandit
- https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning
- https://en.wikipedia.org/wiki/SSS*
- https://en.wikipedia.org/wiki/A*_search_algorithm
- https://en.wikipedia.org/wiki/Simulated_annealingmon
- https://www.chessprogramming.org/MC%CE%B1%CE%B2 (Monte Carlo Alpha-Beta)
- https://en.wikipedia.org/wiki/Beam_search
- https://en.wikipedia.org/wiki/Anytime_algorithm
- https://www.ijcai.org/Proceedings/05/Papers/0596.pdf (BULB, beam with backtracking)
- https://en.wikipedia.org/wiki/Stochastic_scheduling

## Reinforcement Learning

- https://en.wikipedia.org/wiki/Reinforcement_learning
- http://incompleteideas.net/book/the-book-2nd.html
- https://learn.udacity.com/courses/ud600
